\documentclass{assignment}
\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage[dvipsnames]{xcolor}
\usepackage{enumitem}
\usepackage{cleveref}
\usepackage{color,soul}
\usepackage{amsfonts}
\newcommand{\indicator}[1]{\mathbf{1}_{\{#1\}}}
\usepackage{xparse}

% Define a new command for PDFs with subscripts
\NewDocumentCommand{\pdf}{ m }{f_{#1}(#1)}
\NewDocumentCommand{\indp}{}{\perp\!\!\!\!\perp}

\def\code#1{\texttt{#1}}
\newtheorem{anm}{Anm}

\begin{document}


\assignmentTitle
{Lucas Frykman}{0210127650}
{SF1930}
{Liam Solus}
{assets/KTH_logga.png}
{Statistisk inlärning och dataanalys}
{Projekt}

% \section*{Introduktion}
% Vi betraktar den skateboard tävlingen.

\section{Uppvärmning}

\begin{figure}[!h]
    \caption{Histogram av betyg skalad mellan 0 och 1}
    \begin{center}
        \includegraphics[width = 99mm]{assets/Figure_2.png} \label{Histogram 1}
    \end{center}
\end{figure}


Låt $B$ vara betyg för en ett skateboardåkare och trick. Vi vill skatta $P(B>0.6|B>0) = \frac{P(B>0|B>0.6)P(B>0.6)}{P(B>0)}=\frac{P(B>0.6)}{P(B>0)}$
som $\tilde{P}(B>0.6|B>0) = \frac{\sum_{i}^4\sum_{j}^{96}trick_{ij}\indicator{[0.6,1]}}{\sum_{i}^4\sum_{j}^{96}trick_{ij}\indicator{[0,1]}} \approx 0.96$
Det här stämmer med utseendet på \cref{Histogram 1}. När man plottar run 2 mot run 1 ser de ut att ha jätte svag korrelation \cref{Spridningsdiagram}

\begin{figure}
    \caption{Spridningsdiagram mellan run 1 och run 2}
    \begin{center}
        \includegraphics[width = 99mm]{assets/Figure_1.png}
    \end{center}
    \label{Spridningsdiagram}
\end{figure}


\section{En frekventistisk modell}
\begin{anm} \label{properties}
    Vår model för $X_i$ är följande 
    \\ $X_i = \left\{\begin{matrix}
        0 \text{ om } V_i=0
        \\ Z_i \text{ om } V_i=1
    \end{matrix}\right.$
    \\ där $V_i \sim \text{Ber}(\theta_i)$ och $Z_i\sim \text{Beta}(\alpha_i,\beta_i)$ det här är ekvivalent med att säga
    \\ $V_i = \indicator{x\neq0}(X_i)$ och $Z_i= X_i|(V_i=1)$ 
    \\ eftersom det här är bara en transformation av stokatiska variebler ger stickprov från $X_i$ oss ett stickprov för $Z_i$ och $V_i$
\end{anm}

\subsection*{(a) Skatta $\theta_{i}$}

Låt $x_{i[n]} = (x_{i1},x_{i2},...x_{in})^T$ vara vår stickprov från samtliga trick skateboardåkaren $i$ utförde.
\begin{align}
    L(\theta_{i},\alpha_{i},\beta_{i}|x_{i[n]}) = \prod_{j=1}^nf_{x_{i}}(x_{ij}) = \prod_{j}^n (1-\theta_i) \indicator{x=0} (x_{ij}) + \theta_if_{Z_i}(x_{ij})\indicator{x\neq0}(x_{ij})
    \\ \Longleftrightarrow \nonumber
    \\ L(\theta_{i},\alpha_{i},\beta_{i}|x_{i[n]}) = (1-\theta_i)^{n-m}\theta_i^{m}\prod_{j=1}^n\left(f_{Z_i}(x_{ij})\indicator{x\neq0}(x_{ij}) + \indicator{x=0}(x_{ij})\right) 
\end{align}
där $m=\sum_{j=1}^n \indicator{x\neq0}(x_{ij})$ alltså hur många gånger $x_{i}$ inte är noll (gånger tävlaren $i$ landade tricket).
Nu tar vi log likliehoodfunktionen.
\begin{align}
    \Longrightarrow \log(L) = (n-m)\log(1-\theta_i)+m\log(\theta_i)+ \sum_{j=1}^n \log \left(f_{Z_i}(x_{ij})\indicator{x\neq0}(x_{ij}) + \indicator{x=0}(x_{ij})\right) \label{loglike}
    \\ \Longleftrightarrow \partial_{\theta_i}\log(L) = \frac{m-n}{1-\theta_i} + \frac{m}{\theta_i} = 0 \label{blah}
    \\ \Longleftrightarrow \frac{m-n\theta_i}{\theta_i(1-\theta_i)} = 0 \Longleftrightarrow \hat{\theta_i} = \frac{m}{n} \label{MLE result}
\end{align}
MLE för bernoulli fördelningens $V_i$ parameter $\hat{\theta_i} = \text{argmax}_{\theta\in\Omega} L(\theta_i|v_{i[n]}) = \bar{v_i}$ skulle ge oss samma resultat. Eftersom vi kan transformera stickprovet $x_{i[n]}\rightarrow v_{i[n]}$ med \cref{properties} $v_i=\indicator{x\neq0}(x_i)$.
vilket betyder att $m=\sum_{j=1}^n v_i$ och därmed får \cref{MLE result} att sammanfalla med MLE av bernoulli fördelningen. 
\subsection*{(b) skatta $\alpha_i$ och $\beta_i$}
Observera att från \cref{loglike}  $\sum_{j=1}^n \log \left(f_{Z_i}(x_{ij})\indicator{x\neq0}(x_{ij}) + \indicator{x=0}(x_{ij})\right) = \sum_{j=1}^n \log \left(f_{Z_i}(x_{ij})\indicator{x\neq0}(x_{ij})\right)$ eftersom $\log(1)=0$.
Vi vet att 
\\ $\text{argmax}_{\alpha,\beta\in\Omega}\log(L)=\text{argmax}_{\alpha,\beta\in\Omega}\sum_{j=1}^n \log \left(f_{Z_i}(x_{ij})\indicator{x\neq0}(x_{ij})\right)$ vilket är ekvivalent med 
\\$\text{argmax}_{\alpha,\beta\in\Omega} \log(L(\alpha,\beta|z_{i[k]}))$ för att $z$ stickprovet innehåller alla trick som landade $z_{i[k]}=(z_{i1},\dots z_{ik})^T = \left\{x_{ij}\in x_{i[n]} : x_{ij}\neq 0\right\}$
\\ Vi ska alltså bara maximera log-likelihood av beta fördelningens paramtrerna givet data from $Z_i$

\begin{align*}
    \left\{\begin{matrix}
        \partial_{\alpha}\log(L(\alpha,\beta|z_{i[k]})) = \sum_{j=1}^k \partial_\alpha \log(f(z_{ij})) = 0
        \\ \\ \partial_{\beta}\log(L(\alpha,\beta|z_{i[k]})) = \sum_{j=1}^k \partial_\beta \log(f(z_{ij})) = 0
        \end{matrix}\right.
    \\ 
    \\ \text{:: } \partial_\alpha\log(f(z_{ij})) = \partial_\alpha\log \left( \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} \cdot z_{ij}^{\alpha-1} \cdot (1-z_{ij})^{\beta-1}\right) 
    \\  = \partial_\alpha \left(\log \Gamma(\alpha + \beta) - \log\Gamma(\alpha) - \log \Gamma(\beta) + (\alpha-1) \log z_{ij} + (\beta-1)\log (1-z_{ij}) \right)
    \\  = \partial_\alpha\log(f(z_{ij})) = \psi(\alpha+\beta) - \psi(\alpha) + \log z_{ij} \text{  där  } \psi = \Gamma ' / \Gamma 
    \\ \left(\text{Vi gör liknande för } \partial_\beta \log f(z_{ij}) \right)
    \\ \Rightarrow 
    \left\{\begin{matrix}
        \partial_{\alpha}\log L= k\psi(\alpha+\beta)-k\psi(\alpha) + \sum_{j=1}^k\log(z_{ij}) = 0
        \\ \\ \partial_{\beta}\log L= k\psi(\alpha+\beta)-k\psi(\beta) + \sum_{j=1}^k\log(1-z_{ij})) = 0
        \end{matrix}\right.
\end{align*}
\\ Det går dock inte att lösa ML skattningen analytisk härifrån. Numeriska metoder som newton rhapson eller gradient descent
behövs för att skatta vår ML skattning. Att göra så medför sig en del problem som ökar systematiska felet genom
numerisk fel. Vi behåller riktighet i punktskattningen genom att använda moment metoden istället. Vi utgår från följande
system ekvationerna
\begin{align*}
    \left\{\begin{matrix}
        M_1(\mathbf{Z}_i)  = \text{E}[Z_i]
        \\ M_2(\mathbf{Z}_i) = \text{Var}[Z_i]+\text{E}[Z_i]^2
    \end{matrix}\right.
    \Longleftrightarrow \left\{\begin{matrix}
        \Leftrightarrow M_1 = \frac{\alpha}{\alpha+\beta}
        \\ M_2 = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}+(\frac{\alpha}{\alpha+\beta})^2
    \end{matrix}\right.
    \\ :: S^2=\frac{1}{n}\sum (Z_{k}-\bar{Z})^2 = M_2-M_1^2
    \\ \Longleftrightarrow \left\{\begin{matrix}
        \Leftrightarrow \alpha = \beta\frac{M_1}{1-M_1}
        \\ S^2 = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
    \end{matrix}\right.
    \Longleftrightarrow \left\{\begin{matrix}
        \widetilde{\alpha} = M_1\left(\frac{M_1(1-M_1)}{S^2}-1\right)
        \\ \widetilde{\beta} = (1-M_1)\left(\frac{M_1(1-M_1)}{S^2}-1\right)
    \end{matrix}\right.
    \\ = \left\{\begin{matrix}
        \widetilde{\alpha_i} = \bar{Z_i}\left(\frac{\bar{Z_i}(1-\bar{Z_i})}{S^2}-1\right)
        \\ \widetilde{\beta_i} = (1-\bar{Z_i})\left(\frac{\bar{Z_i}(1-\bar{Z_i})}{S^2}-1\right)
    \end{matrix}\right.
\end{align*}
För vissa skatebordåkare kan man inte få en punktskattningen eftersom det finns endast en
datapunkt $z_1$ för vissa skatebordåkare. Stickprovsvariansen i sådana fall blir lika med noll. Stickprovsvariansen
representerar hur osäkert vi är om en skatebordåkares prestation på en trick. Intuivt sett Vi gör valet
här att skatta stickprovsvariansen som $S_i^2\approx \bar{S^2}$ dvs vi tar medelvärdet av samtliga varianser. 
\subsection*{(c) Model för $Y_i$}
$Y_i$ ska vara run betyget för åkaren $i$. Eftersom $Y_i\in(0,1]$ (varje deltagare får betyg större än 0) kommer bernoulli delen försvinna. Så vi antar att $Y_i\sim \text{Beta}(\alpha_i,\beta_i)$. 
Vi använder samma metod som vi använde för att skatta $\alpha,\beta$ för $X$.
\subsection*{(d) Simulering}
Total betyget för varje deltagare beräknas som summan av deras två största trick betyg och största run betyg. Vi
kan beskriva i termer av stokatiska variabler. Låt $O_i$ vara total betyg för deltagare $i$.
Låt $Q_{i,\text{först}}=\max(X_{i1},X_{i2},X_{i3},X_{i4})$ och 
\\$Q_{i,\text{andra}}=\max(\min(X_{i1}, X_{i2}), \min(X_{i1}, X_{i3}), \min(X_{i1}, X_{i4}), \min(X_{i2}, X_{i3}), \min(X_{i2}, X_{i4}), \min(X_{i3}, X_{i4}))$.
\\Vi vill simulera total betyget för varje deltagare $i$ som $O_i=Q_{i,\text{först}}+Q_{i,\text{andra}}+\max(Y_{i1},Y_{i2})$
De som fick de 4 högsta betyg får delta i finalen. Vi simulerar $5000$ LCQ:ar. Det ger oss en följd av stokatisk sets $\mathbf{W}_1,...\mathbf{W}_{5000}$.
Python har redan libraries för att generera stickprov från beta och bernoulli fördelningar. Så vi slipper 
använda box muller, inverse metoden, eller dylikt. 
\begin{figure}
    \caption{Frequency appearing in final $\mathbf{W}$}
    \begin{center}
        \includegraphics[width = 200mm]{assets/freq_bar_graph.png} 
    \end{center}
    \label{Histogram 2}
\end{figure}
Jag skapade en frequency bar graph för att visualisera simuleringen och fick detta i en körning \cref{Histogram 2}.
De som har markerats i orange är de som faktiskt vann den verkliga LCQ:en.
Typvärdet på $\mathbf{W}$ innehöll oftast Hoban, Eaton, Jordan, och Shirai. Typvärdets frekvens vara kring 50 gger dvs $1\%$. 
Frekvensen av när $\mathbf{W}$ innehöll samtliga verkliga vinnare (nämligen Gustavo, Hoban, Eaton, Decenzo) vara när 16-20 gger dvs mindre än $0.5\%$.  

\newpage 
\section{En bayesiansk modell}
\subsection*{(a) Apriori fördelnignar} 
$\pdf{\theta_i}\propto 1$
\\ $\pdf{\alpha_i,\beta_i} = \frac{\lambda^\theta}{\Gamma(\theta)}(\alpha_i+\beta_i+1)^{\theta-1}e^{-\lambda(\alpha_i+\beta_i+1)}(\alpha_i+\beta_i)^{-1}$  
\\ Vi antar $\Theta_i \indp A_i,B_i$ för alla $i$ $\Longrightarrow \pdf{\theta_i,\alpha_i,\beta_i} = \pdf{\theta_i}\pdf{\alpha_i,\beta_i}$ 
\subsection*{(b) Aposteriori för $X_i$ och skattning} 
$\pdf{\theta_i, \alpha_i,\beta_i|x_i} \propto \pdf{\theta_i}\pdf{\alpha_i,\beta_i} \pdf{x_i|\theta_i,\alpha_i,\beta_i}$
\\ Vårt mål är att använda aposteriorin för att skatta $\text{E}[\Theta_i,A_i,B_i|\mathbf{X_i}=\mathbf{x_i}]$.
För att göra detta använder vi metropolis algoritimen för att generera ett stickprov och ta stickprovsmedelvärdet
$$
\begin{pmatrix} \theta_{i0},\theta_{i1} \dots \theta_{i5000}\\ \alpha_{i0},\alpha_{i1}\dots \alpha_{i5000}  \\ \beta_{i0},\beta_{i1}\dots \beta_{i5000}\end{pmatrix} \longrightarrow
\begin{pmatrix}
    \bar{\theta_i} \\
    \bar{\alpha_i} \\
    \bar{\beta_i}
\end{pmatrix}
$$  
Aposteriorin är vår målfördelning men vi kommer använda $L_{p}(\theta_i, \alpha_i,\beta_i) \propto \log(\pdf{\theta_i, \alpha_i,\beta_i|\mathbf{x}_i})$ istället.
Data fördelning $\pdf{\mathbf{x}_i|\theta_i, \alpha_i,\beta_i}$ i aposterorin är samma som likelihoodfunktionen 
$L(\theta_i, \alpha_i,\beta_i|\mathbf{x}_i)$
% $$
% \Rightarrow L_p(\theta_i, \alpha_i,\beta_i) = \underbrace{\log(\pdf{\theta_i)}}_{\log(1)=0}+\log(\pdf{\alpha_i,\beta_i})+\log L(\theta_i,\alpha_i,\beta_i)
% $$
 
% \begin{enumerate}
%     \item Välj $[\theta_{i0},\beta_{i0},\alpha_{i0}]^T$
%     \item Generera utfall från förslag fördelning $ Lambda \sim q(\Lambda|\Psi_{i-1})$
%     \item Sätt $R = L_p(\Lambda)/f(\Psi_{k-1})$
%     \item Generera $U\sim \text{U}(0,1)$
%     \item Om $U \leq \min(1,R)$, sätt $\Psi_{k}=\Lambda$
%     \item   
% \end{enumerate}
\subsection*{(c) Aposteriori för $Y_i$}
\subsection*{(d) Simulering}
\subsection*{(e)}
\section{En bayesiansk modell med en hierarki}
\subsection*{(a) Apriori fördelning för $\theta$} 
\subsection*{(b) Aposteriori för $X_i$}
\subsection*{(c) Aposteriori för $Y_i$}
\subsection*{(d) Simulering}
\subsection*{(e) Grafisk model}
\section{Diskussion}

% \section{Kod}
% \lstinputlisting[language=Matlab,caption=foo]{assets/tmp.m} 
\end{document}
